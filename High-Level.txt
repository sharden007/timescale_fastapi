Here is a **high-level bulleted list** of the steps to set up Kafka to produce and consume sensor data every 30 seconds, store it in a PostgreSQL database, and verify that Kafka is running on Windows:


- **Install Required Software**:
  - Install **Java** (required for Kafka). Note Java will have ot be installed for WSL env as well
  - Install **Apache Kafka** and extract it to a local directory.
  - Install **PostgreSQL** for the database.

- **Start Kafka and Zookeeper**:
  - Start **Zookeeper**: from Kafaka directory run:  `.\bin\windows\zookeeper-server-start.bat .\config\zookeeper.properties`.
  - Start **Kafka**: from Kafaka directory run:  `.\bin\windows\kafka-server-start.bat .\config\server.properties`.
  - Verify Kafka is running by listing topics: `.\bin\windows\kafka-topics.bat --list --bootstrap-server localhost:9092`.

- **Create PostgreSQL Database and Table**:
  - Create a database named `sensor_data_db`.
  - Create a table `sensor_data` with columns (`sensor_id`, `value`, `time`).

- **Set Up Python Environment**:
  - Install required Python packages: `confluent_kafka`, `asyncpg`, `loguru`.

- **Configure and Run the Producer Script (from src directory)  (`kafka_producer.py`)**:
  - Ensure the producer script sends sensor data (with a timestamp) to the Kafka topic (`sensor_data`) every 30 seconds.
  - Run the producer script: `python kafka_producer.py`.

- **Configure and Run the Consumer Script(from src directory)  (`kafka_consumer.py`)**:
  - Ensure the consumer script subscribes to the Kafka topic, consumes messages, converts the timestamp from string to `datetime`, and inserts data into PostgreSQL.
  - Run the consumer script: `python kafka_consumer.py`.

- **Verify Data Insertion in PostgreSQL**:
  - Query the PostgreSQL table to verify that sensor data is being inserted correctly: 
    ```sql
    SELECT * FROM sensor_data;
    ```

